{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from diffsynth import ModelManager, WanVideoPipeline, save_video, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = Path(\"dataset/train\")\n",
    "metadata = pd.read_csv(\"dataset/metadata.csv\")\n",
    "masks = Path(\"dataset/lip_masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latents\n",
    "latents_paths = [video_path for video_path in vid_path.iterdir() if video_path.suffix == \".pth\"]\n",
    "# shuffle(latents_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C, F, H, W\n",
    "latent = torch.load(latents_paths[0], map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Add audio to list\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     audio_list\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m: audio_data,\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m: caption\n\u001b[1;32m     23\u001b[0m     })\n\u001b[0;32m---> 25\u001b[0m latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(latents)\u001b[38;5;241m.\u001b[39mto(\u001b[43mpipe\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Load 5 latents\n",
    "latents = []\n",
    "audio_list = []\n",
    "\n",
    "for latent_path in latents_paths[12:14]:\n",
    "    latents.append(torch.load(latent_path)[\"latents\"])\n",
    "    name = latent_path.name.replace(\".tensors.pth\", \"\")\n",
    "\n",
    "    caption = metadata[metadata[\"file_name\"] == name][\"text\"].values[0]\n",
    "    # Load audio\n",
    "    audio_path = f\"dataset/audio/{name.replace('.mp4', '.wav')}\"\n",
    "\n",
    "    print(os.path.exists(audio_path))\n",
    "\n",
    "    audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # Add audio to list\n",
    "    \n",
    "    audio_list.append({\n",
    "        'name': name,\n",
    "        'audio': audio_data,\n",
    "        'caption': caption\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.stack(latents).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "from IPython.display import Audio\n",
    "display(Audio(audio_list[1]['audio'], rate=41000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.vae.to(pipe.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pipe.vae.decode(latents, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    video = out[i].permute(1, 2, 3, 0).cpu().float()\n",
    "    save_video(video, f\"video_{i}.mp4\", fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
