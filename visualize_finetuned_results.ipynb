{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffsynth import ModelManager, WanVideoPipeline, save_video, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = \"models/lip_finetuned/wandb/diffsynth-studio/n1eqrg1d/checkpoints/model-step=700.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager(torch_dtype=torch.bfloat16, device=\"cpu\")\n",
    "model_manager.load_models([\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\",\n",
    "])\n",
    "model_manager.load_lora(trained_model_path, lora_alpha=1.0)\n",
    "pipe = WanVideoPipeline.from_model_manager(model_manager, device=\"cuda\")\n",
    "pipe.enable_vram_management(num_persistent_param_in_dit=None)\n",
    "\n",
    "video = pipe(\n",
    "    prompt=\"i was telling my friend about how much i wanted to see him\",\n",
    "    negative_prompt=\"low quality, unclear facial expressions, blurry\",\n",
    "    num_inference_steps=50,\n",
    "    seed=0, tiled=True\n",
    ")\n",
    "save_video(video, \"video.mp4\", fps=30, quality=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FPFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\n",
      "    model_name: wan_video_dit model_class: WanModel\n",
      "        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 16, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    The following models are loaded: ['wan_video_dit'].\n",
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n",
      "    model_name: wan_video_text_encoder model_class: WanTextEncoder\n",
      "    The following models are loaded: ['wan_video_text_encoder'].\n",
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n",
      "    model_name: wan_video_vae model_class: WanVideoVAE\n",
      "    The following models are loaded: ['wan_video_vae'].\n",
      "Using wan_video_dit from models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors.\n",
      "Using wan_video_text_encoder from models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.\n",
      "Using wan_video_dit from models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors.\n",
      "Using wan_video_vae from models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.\n",
      "No wan_video_image_encoder models available.\n",
      "No wan_video_motion_controller models available.\n",
      "No wan_video_vace models available.\n"
     ]
    }
   ],
   "source": [
    "model_manager = ModelManager(torch_dtype=torch.bfloat16, device=\"cpu\")\n",
    "model_manager.load_models([\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\",  # Load base DiT first\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\",\n",
    "])\n",
    "\n",
    "# Get the DiT model from the manager\n",
    "dit_model = model_manager.fetch_model(\"wan_video_dit\")\n",
    "\n",
    "# Now load and filter your fine-tuned checkpoint\n",
    "checkpoint_path = \"models/lip_finetuned/wandb/diffsynth-studio/n1eqrg1d/checkpoints/model-step=700.ckpt\"\n",
    "model_dict = torch.load(checkpoint_path)\n",
    "\n",
    "# Filter out audio-related parameters\n",
    "filtered_dict = {k: v for k, v in model_dict.items() if \"audio\" not in k}\n",
    "\n",
    "# Directly load the filtered weights into the base model\n",
    "# The prefix \"denoising_model.\" is common in these checkpoints\n",
    "model_weights = {k.replace(\"denoising_model.\", \"\"): v for k, v in filtered_dict.items() \n",
    "                 if k.startswith(\"denoising_model.\")}\n",
    "\n",
    "# Load filtered weights into the DiT model\n",
    "dit_model.load_state_dict(model_weights, strict=False)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = WanVideoPipeline.from_model_manager(model_manager, device=\"cuda\")\n",
    "pipe.enable_vram_management(num_persistent_param_in_dit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  A person talking to their friend at a well-lit cafe in the middle of New York. Ana yells at her friend, 'What is it that you've been doing all of this time?', while she looks at Ana's friend with a stern expression.\n",
      "Prompt:  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:47<00:00,  3.35s/it]\n",
      "VAE decoding: 100%|██████████| 9/9 [00:08<00:00,  1.08it/s]\n",
      "Saving video:   0%|          | 0/81 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Saving video: 100%|██████████| 81/81 [00:00<00:00, 314.00it/s]\n"
     ]
    }
   ],
   "source": [
    "video = pipe(\n",
    "    prompt=\"A person talking to their friend at a well-lit cafe in the middle of New York. Ana yells at her friend, 'What is it that you've been doing all of this time?', while she looks at Ana's friend with a stern expression.\",\n",
    "    negative_prompt=\"...\",\n",
    "    num_inference_steps=50,\n",
    "    seed=0, tiled=True\n",
    ")\n",
    "save_video(video, \"video.mp4\", fps=20, quality=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
